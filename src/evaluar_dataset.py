import pandas as pd

def evaluar_dataset(path):
    print(f"\nğŸ“Š Evaluando dataset: {path}")
    df = pd.read_csv(path)
    total_filas, total_columnas = df.shape
    print(f"\nâœ… TamaÃ±o: {total_filas} filas x {total_columnas} columnas")

    # Valores nulos
    nulos = df.isnull().sum()
    columnas_con_nulos = nulos[nulos > 0]
    if not columnas_con_nulos.empty:
        print("\nâš ï¸ Columnas con valores nulos:")
        print(columnas_con_nulos)
    else:
        print("\nâœ… No hay valores nulos.")

    # Tipos de datos
    print("\nğŸ” Tipos de datos:")
    print(df.dtypes)

    # Valores Ãºnicos por columna
    print("\nğŸ“Œ Valores Ãºnicos por columna:")
    for col in df.columns:
        print(f"- {col}: {df[col].nunique()} Ãºnicos")

    # Duplicados
    duplicados = df.duplicated().sum()
    print(f"\nğŸ§­ Filas duplicadas: {duplicados}")

    # RevisiÃ³n de columnas poco Ãºtiles
    columnas_con_pocos_unicos = [col for col in df.columns if df[col].nunique() <= 1]
    if columnas_con_pocos_unicos:
        print("\nâš ï¸ Columnas con un solo valor (poco Ãºtiles):")
        print(columnas_con_pocos_unicos)

    # RevisiÃ³n general
    print("\nğŸ§  EvaluaciÃ³n final:")
    if total_filas < 100:
        print("âŒ Dataset demasiado pequeÃ±o para modelos robustos.")
    elif len(columnas_con_nulos) > total_columnas * 0.3:
        print("âŒ Demasiadas columnas con valores nulos.")
    elif duplicados > total_filas * 0.5:
        print("âŒ Muchos duplicados, podrÃ­an afectar el anÃ¡lisis.")
    else:
        print("âœ… Dataset apto para anÃ¡lisis y modelado inicial.")

if __name__ == "__main__":
    evaluar_dataset("data/processed/datos_limpios.csv")
